{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate l-distance between two strings\n",
    "# accepts two string parameters: \n",
    "def ldistance(strA, strB):\n",
    "    import numpy as np # to create matrix\n",
    "    \n",
    "    #setting up matrix\n",
    "    size_x = len(strA) + 1\n",
    "    size_y = len(strB) + 1\n",
    "    matrix = np.zeros((size_x, size_y))\n",
    "    for x in range(size_x):\n",
    "        matrix [x, 0] = x\n",
    "    for y in range(size_y):\n",
    "        matrix [0, y] = y\n",
    "    \n",
    "    # prepopulation\n",
    "    print(matrix)\n",
    "    \n",
    "    #populating matrix\n",
    "    \n",
    "    # start at 1 so don't hit zero\n",
    "    for x in range(1, size_x):\n",
    "        for y in range(1, size_y):\n",
    "            # then must decrease by 1 because of that\n",
    "            if strA[x-1] == strB[y-1]: #elements are equal\n",
    "                matrix[x][y] = matrix[x-1][y-1]\n",
    "            else: \n",
    "                # the three Levenshtein conditions for min\n",
    "                # since all are adding one at end, can check prior to addition\n",
    "                a = matrix[x][y-1]\n",
    "                b = matrix[x-1][y]\n",
    "                c = matrix[x-1][y-1]\n",
    "                # a is min\n",
    "                if (a <= b and a <= c):\n",
    "                    matrix[x][y] = a + 1\n",
    "                # b is min\n",
    "                elif (b <= a and b <= c):\n",
    "                    matrix[x][y] = b + 1\n",
    "                # c is min\n",
    "                else:\n",
    "                    matrix[x][y] = c + 1\n",
    "    # postpopulation\n",
    "    print(matrix)\n",
    "                    \n",
    "    # bottom-right element is l-distance\n",
    "    distance = matrix[-1][-1]\n",
    "    \n",
    "    # distance\n",
    "    print(distance)\n",
    "    \n",
    "    # taking into consideration the length of the words\n",
    "    # normalize based on longest word\n",
    "    length = 0\n",
    "    if ((len(strA) > len(strB)) or (len(strA) == len(strB))):\n",
    "        length = len(strA)\n",
    "    else:\n",
    "        length = len(strB)\n",
    "        \n",
    "    lscore = ((length - distance) / length) \n",
    "    \n",
    "    return lscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 2. 3. 4. 5. 6. 7.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [2. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [3. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [4. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [5. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 2. 3. 4. 5. 6. 7.]\n",
      " [1. 1. 2. 3. 4. 5. 6. 7.]\n",
      " [2. 2. 2. 2. 3. 4. 5. 6.]\n",
      " [3. 3. 3. 3. 2. 3. 4. 5.]\n",
      " [4. 4. 4. 4. 3. 2. 3. 4.]\n",
      " [5. 5. 5. 5. 4. 3. 2. 3.]]\n",
      "3.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5714285714285714"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldistance(\"hello\",\"smellow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove vowels, common words, numbers, and punctuation, as well as stem the words\n",
    "# accepts 2 parameters: string for sample and string for language\n",
    "def clean_sample(sample, language):\n",
    "    import nltk \n",
    "    import re # to use in regular expressions\n",
    "    from nltk.stem.snowball import SnowballStemmer # to stem words, doesn't work well\n",
    "    from nltk.corpus import stopwords # to remove common words\n",
    "    \n",
    "    # creating objects for stemmer, common words and key for punctuation/numbers to be removed\n",
    "    stemmer = SnowballStemmer(str(language))\n",
    "    common_words = stopwords.words(language)\n",
    "    punct = re.compile(r\"[,@\\'?\\.$%_/:()]\")\n",
    "    numbers = re.compile(r\"[0-9]\")\n",
    "\n",
    "    # separates sentence into elements and stores in elemList\n",
    "    sentences=[]\n",
    "    elemList=sample.split()\n",
    "\n",
    "    #removes punctuation, numbers, common words and stores in wordList\n",
    "    wordList=[]\n",
    "    for i in range(len(elemList)):\n",
    "        s = elemList[i]\n",
    "        find = re.search(punct, s)\n",
    "        findNum = re.search(numbers, s)\n",
    "        if(find == None and findNum == None):\n",
    "            if s not in common_words:\n",
    "                wordList.append(s)\n",
    "\n",
    "    # stems words and appends them to list to be returned\n",
    "    stems=[]\n",
    "    for word in wordList:\n",
    "        w = stemmer.stem(word)\n",
    "        stems.append(w)\n",
    "        \n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes vowels\n",
    "# parameter: list of words from sample\n",
    "def remove_vowels(wordList):\n",
    "    import re # to use in regular expressions\n",
    "    vowels = re.compile(r\"a|e|i|o|u\") # key to find vowels - differs between languages\n",
    "    \n",
    "    # omitting vowels from words\n",
    "    for x in range(len(wordList)):\n",
    "        w = \"\"\n",
    "        for i in range(len(wordList[x])):\n",
    "            find = re.search(vowels, wordList[x][i:i+1])\n",
    "            if find == None:\n",
    "                w += wordList[x][i:i+1]\n",
    "        wordList[x] = w\n",
    "        \n",
    "    return wordList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that lines up pairs of words between the two samples, and calculates their l-distance\n",
    "# parameters: 2 lists of words from each sample, 1 list of manually-created alignment between the two lists\n",
    "def measure_samples(list1, list2, alignment):\n",
    "    distances=[]\n",
    "    \n",
    "    # aligning pairs\n",
    "    for i in range(len(alignment)):\n",
    "        idx1 = alignment[i][0]\n",
    "        idx2 = alignment[i][1]\n",
    "        str1 = list1[idx1]\n",
    "        str2 = list2[idx2]\n",
    "        print(str1 + \"-\" + str2)\n",
    "        l = ldistance(str(str1), str(str2))\n",
    "        distances.append(l)\n",
    "        \n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to concatenate all consonants in the sample\n",
    "# parameters: a list of all words in a sample\n",
    "def concat_all(wordList):\n",
    "    fullString=\"\"\n",
    "    for word in wordList:\n",
    "        fullString += word\n",
    "        \n",
    "    return fullString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spa-Eng Pairs\n",
      "\n",
      "gd-g\n",
      "prps-prpst\n",
      "prncpl-prncp\n",
      "chrtr-crt\n",
      "nt-ncn\n",
      "xprss-xprs\n",
      "prtclr-prtcl\n",
      "nd-ncs\n",
      "chv-lgr\n",
      "ntrn-ntrncnl\n",
      "cpr-cpr\n",
      "prmt-prmv\n",
      "ncrg-lnt\n",
      "rspct-rspt\n",
      "hmn-hmn\n",
      "rght-drch\n",
      "fndmnt-fndmntl\n",
      "frdm-lbrtd\n",
      "dstnct-dstncn\n",
      "\n",
      "\n",
      "English consonants\n",
      "\n",
      "['gd', 'prps', 'prncpl', 'chrtr', 'nt', 'xprss', 'prtclr', 'nd', 'chv', 'ntrn', 'cpr', 'prmt', 'ncrg', 'rspct', 'hmn', 'rght', 'fndmnt', 'frdm', 'wtht', 'dstnct']\n",
      "\n",
      "\n",
      "Spanish consonants\n",
      "\n",
      "['g', 'prpst', 'prncp', 'crt', 'ncn', 'xprs', 'prtcl', 'ncs', 'lgr', 'cpr', 'ntrncnl', 'prmv', 'lnt', 'rspt', 'drch', 'hmn', 'lbrtd', 'fndmntl', 'dstncn']\n",
      "\n",
      "\n",
      "L-Distance measures per pair of aligned words\n",
      "\n",
      "[0.5, 0.8, 0.8333333333333334, 0.6, 0.3333333333333333, 0.8, 0.8333333333333334, 0.3333333333333333, 0.0, 0.5714285714285714, 1.0, 0.75, 0.0, 0.8, 1.0, 0.25, 0.8571428571428571, 0.2, 0.8333333333333334]\n",
      "\n",
      "\n",
      "Average\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5944862155388472"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate import Alignment\n",
    "engSample = \"Guided by the purposes and principles of the Charter of the United Nations, and expressing in particular the need to achieve international cooperation in promoting and encouraging respect for human rights and fundamental freedoms for all without distinction\"\n",
    "spaSample = \"Guiado por los propósitos y principios de la Carta de las Naciones Unidas, y expresando en particular la necesidad de lograr la cooperación internacional para promover y alentar el respeto de los derechos humanos y las libertades fundamentales para todos sin distinción\"\n",
    "\n",
    "#Removing vowels, punctuation, stemming\n",
    "engWords = clean_sample(engSample, \"english\")\n",
    "spaWords = clean_sample(spaSample, \"spanish\")\n",
    "engWords = remove_vowels(engWords)\n",
    "spaWords = remove_vowels(spaWords)\n",
    "\n",
    "#Manually transcribing sentence alignment to compare word-to-word\n",
    "print(\"Spa-Eng Pairs\\n\")\n",
    "eng_spa_align = [(0,0), (1,1), (2,2), (3,3), (4,4), (5,5), (6,6), (7,7), (8,8), (9, 10), (10, 9), (11,11), (12, 12), (13, 13), (14, 15), (15, 14), (16, 17), (17, 16), (19, 18)]\n",
    "\n",
    "\n",
    "#Passing lists of consonant-only words in both languages and their alignment to Ldistnance method\n",
    "measures = measure_samples(engWords, spaWords, eng_spa_align)\n",
    "print(\"\\n\")\n",
    "print(\"English consonants\\n\")\n",
    "print(engWords)\n",
    "print(\"\\n\")\n",
    "print(\"Spanish consonants\\n\")\n",
    "print(spaWords)\n",
    "print(\"\\n\")\n",
    "print(\"L-Distance measures per pair of aligned words\\n\")\n",
    "print(measures)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Average\")\n",
    "avg = 0\n",
    "for i in range(len(measures)):\n",
    "    avg = avg + measures[i]\n",
    "avg/len(measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenation of all English consonants\n",
      "gdprpsprncplchrtrntxprssprtclrndchvntrncprprmtncrgrspcthmnrghtfndmntfrdmwthtdstnct\n",
      "\n",
      "\n",
      "Concatenation of all Spanish consonants\n",
      "gprpstprncpcrtncnxprsprtclncslgrcprntrncnlprmvlntrsptdrchhmnlbrtdfndmntldstncn\n",
      "\n",
      "\n",
      "Distance between the two concatenations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate import Alignment\n",
    "engSample = \"Guided by the purposes and principles of the Charter of the United Nations, and expressing in particular the need to achieve international cooperation in promoting and encouraging respect for human rights and fundamental freedoms for all without distinction\"\n",
    "spaSample = \"Guiado por los propósitos y principios de la Carta de las Naciones Unidas, y expresando en particular la necesidad de lograr la cooperación internacional para promover y alentar el respeto de los derechos humanos y las libertades fundamentales para todos sin distinción\"\n",
    "engWords = clean_sample(engSample, \"english\")\n",
    "spaWords = clean_sample(spaSample, \"spanish\")\n",
    "engWords = remove_vowels(engWords)\n",
    "spaWords = remove_vowels(spaWords)\n",
    "\n",
    "print(\"Concatenation of all English consonants\")\n",
    "\n",
    "print(concat_all(engWords))\n",
    "print(\"\\n\")\n",
    "print(\"Concatenation of all Spanish consonants\")\n",
    "print(concat_all(spaWords))\n",
    "print(\"\\n\")\n",
    "print(\"Distance between the two concatenations\")\n",
    "ldistance(engWords, spaWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
