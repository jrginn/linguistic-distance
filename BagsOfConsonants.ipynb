{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove vowels, common words, numbers, and punctuation, as well as stem the words\n",
    "# accepts 2 parameters: string for sample and string for language\n",
    "def clean_sample(sample, language):\n",
    "    import nltk \n",
    "    import re # to use in regular expressions\n",
    "    from nltk.stem.snowball import SnowballStemmer # to stem words, doesn't work well\n",
    "    from nltk.corpus import stopwords # to remove common words\n",
    "    \n",
    "    # creating objects for stemmer, common words and key for punctuation/numbers to be removed\n",
    "    stemmer = SnowballStemmer(str(language))\n",
    "    common_words = stopwords.words(language)\n",
    "    toRemove = re.compile(r\"[aeiou0-9,@\\?\\.$%_/:() ]\")\n",
    "\n",
    "    # separates sentence into elements and stores in elemList\n",
    "    elemList=sample.split()\n",
    "    \n",
    "    # stemming first\n",
    "    stems=[]\n",
    "    for elem in elemList:\n",
    "        if elem not in common_words:\n",
    "            w = stemmer.stem(elem)\n",
    "            stems.append(w)\n",
    "\n",
    "    #now removing punctuation, numbers, vowels and storing in wordList\n",
    "    wordList=[]\n",
    "    for i in stems:\n",
    "        s = i\n",
    "        elem = re.sub(toRemove, \"\", s.lower())\n",
    "        elem = unidecode.unidecode(elem)\n",
    "        # NEW NOT IN LD CODE YET: USE OF UNIDECODER\n",
    "        wordList.append(elem)\n",
    "    \n",
    "    # stems words and appends them to list to be returned\n",
    "    return wordList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a dictionary of form {consonant : count}\n",
    "def get_c_dict(sample,language):\n",
    "    # initialize an empty dictionary\n",
    "    c_dict = {}\n",
    "    # strip the string input to only consonants\n",
    "    # first clean sample (returns a list), use join to turn to string\n",
    "    striped_c = (\"\").join(clean_sample(sample,language))\n",
    "    \n",
    "    # populate the dictionary\n",
    "    for c in striped_c:\n",
    "        if c in c_dict.keys():\n",
    "            # increment count\n",
    "            c_dict[c] +=1\n",
    "        else:\n",
    "            # add to consonant dict\n",
    "            c_dict[c] = 1\n",
    "    \n",
    "    return c_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a dictionary of the differences for each found consonant\n",
    "\n",
    "def get_diff_dict(sampleA,sampleB,langA,langB):\n",
    "    # initialize empty dictionary\n",
    "    diff_dict = {}\n",
    "    \n",
    "    # strip both strings\n",
    "    c_dictA = get_c_dict(sampleA,langA)\n",
    "    c_dictB = get_c_dict(sampleB,langB)\n",
    "    \n",
    "    # get all unique consonants found between both strings\n",
    "    # add both keys, then take the set of them\n",
    "    # set returns all individual elements, so we only get uniques\n",
    "    found_consonants = list(c_dictA.keys()) + list(c_dictB.keys())\n",
    "    uniq_consonants = set(found_consonants)\n",
    "    \n",
    "    for c in uniq_consonants:\n",
    "        if(c in c_dictA and c in c_dictB):\n",
    "            # a consonant appears in both strings\n",
    "            # thus can properly subtract them\n",
    "            # take absolute value so we can normalize later based on some factor\n",
    "            difference = abs(c_dictA[c] - c_dictB[c])\n",
    "            diff_dict[c] = difference\n",
    "            \n",
    "        elif(c in c_dictA):\n",
    "            # unique consonant only in one string, cannot subtract\n",
    "            # thus the difference is the number of appearances of unique consonant in the one string\n",
    "            diff_dict[c] = c_dictA[c]\n",
    "        else:\n",
    "            diff_dict[c] = c_dictB[c]\n",
    "            \n",
    "    return diff_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C-dictionary str1:\n",
      "{'g': 3, 'd': 5, 'p': 9, 'r': 14, 's': 6, 'n': 12, 'c': 8, 'l': 2, 'h': 5, 't': 13, 'x': 1, 'v': 1, 'm': 4, 'f': 2, 'w': 1}\n",
      "C-dictionary str2\n",
      "{'g': 2, 'p': 9, 'r': 12, 's': 6, 't': 9, 'n': 14, 'c': 9, 'd': 5, 'x': 1, 'l': 6, 'm': 3, 'v': 1, 'h': 2, 'b': 1, 'f': 1}\n",
      "Differences:\n",
      "{'r': 2, 'c': 1, 't': 4, 'x': 0, 'n': 2, 'w': 1, 'm': 1, 'g': 1, 'v': 0, 'd': 0, 'h': 3, 'l': 4, 'b': 1, 'f': 1, 's': 0, 'p': 0}\n"
     ]
    }
   ],
   "source": [
    "# Current issues: accents on string, make sure the regex works (probably should switch to re.sub)\n",
    "# Not sure if dictionary is best. I used it out of simplicity, but when if we ever wanted to iterate a list of lists\n",
    "# could be better. I think performance would be similar, but the benefit of a dictionary is that you can simply call \n",
    "# d[\"s\"] to get all of the differences for the letter \"s\".\n",
    "\n",
    "# Ideas:\n",
    "# To normalize, we could loop through the dictionary keys to add up the differences, then use some\n",
    "# factor to get the difference between 0-1, similar to LD normalization\n",
    "str1 = \"Guided by the purposes and principles of the Charter of the United Nations, and expressing in particular the need to achieve international cooperation in promoting and encouraging respect for human rights and fundamental freedoms for all without distinction\"\n",
    "str2 = \"Guiado por los propósitos y principios de la Carta de las Naciones Unidas, y expresando en particular la necesidad de lograr la cooperación internacional para promover y alentar el respeto de los derechos humanos y las libertades fundamentales para todos sin distinción\"\n",
    "\n",
    "print(\"C-dictionary str1:\")\n",
    "print(get_c_dict(str1,\"english\"))\n",
    "print(\"C-dictionary str2\")\n",
    "print(get_c_dict(str2,\"spanish\"))\n",
    "print(\"Differences:\")\n",
    "print(get_diff_dict(str1,str2,\"english\",\"spanish\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
